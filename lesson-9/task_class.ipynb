{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec4ecbd3-61d8-4304-8ff3-1967d8d24ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "class neural():\n",
    "        \n",
    "    def __init__(self, layers, activations, optimizer, loss, metrics):\n",
    "        if not isinstance(layers, list) or len(layers) == 0:\n",
    "            raise TypeError(\"layers error\")\n",
    "        if not isinstance(activations, list) or len(activations) == 0:\n",
    "            raise TypeError(\"activations error\")\n",
    "        if not isinstance(optimizer, str) or len(optimizer) == 0:\n",
    "            raise TypeError(\"optimizer error\")\n",
    "        if not isinstance(loss, str) or len(loss) == 0:\n",
    "            raise TypeError(\"loss error\")\n",
    "        if not isinstance(metrics, list) or len(metrics) == 0:\n",
    "            raise TypeError(\"metrics error\")\n",
    "\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        self.model = None\n",
    "\n",
    "    def build_model(self, input_dim):\n",
    "        model = keras.Sequential()\n",
    "        \n",
    "        for i in range(len(self.layers)):\n",
    "            if i == 0:\n",
    "                model.add(keras.layers.Dense(units=self.layers[i], activation=self.activations[i], input_shape=(input_dim,)))\n",
    "            else:\n",
    "                model.add(keras.layers.Dense(units=self.layers[i], activation=self.activations[i]))\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def compile(self):\n",
    "        self.model.compile(optimizer=self.optimizer, loss=self.loss, metrics=self.metrics)\n",
    "\n",
    "    def fit(self, x_train, y_train, batch_size, epochs, val_data):\n",
    "        history = self.model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=val_data)\n",
    "        return history\n",
    "\n",
    "    def evaluate(self, x, y):\n",
    "        return self.model.evaluate(x, y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3e186f4-1d2c-44e0-8116-6198035df44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "url = os.getenv(\"local_paris_ds\")\n",
    "\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d16cedeb-c339-4986-a3d1-a7decb44adb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 17 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   squareMeters       10000 non-null  int64  \n",
      " 1   numberOfRooms      10000 non-null  int64  \n",
      " 2   hasYard            10000 non-null  int64  \n",
      " 3   hasPool            10000 non-null  int64  \n",
      " 4   floors             10000 non-null  int64  \n",
      " 5   cityCode           10000 non-null  int64  \n",
      " 6   cityPartRange      10000 non-null  int64  \n",
      " 7   numPrevOwners      10000 non-null  int64  \n",
      " 8   made               10000 non-null  int64  \n",
      " 9   isNewBuilt         10000 non-null  int64  \n",
      " 10  hasStormProtector  10000 non-null  int64  \n",
      " 11  basement           10000 non-null  int64  \n",
      " 12  attic              10000 non-null  int64  \n",
      " 13  garage             10000 non-null  int64  \n",
      " 14  hasStorageRoom     10000 non-null  int64  \n",
      " 15  hasGuestRoom       10000 non-null  int64  \n",
      " 16  price              10000 non-null  float64\n",
      "dtypes: float64(1), int64(16)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91832406-7490-4658-9ca6-d2568e275283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "squareMeters         0\n",
       "numberOfRooms        0\n",
       "hasYard              0\n",
       "hasPool              0\n",
       "floors               0\n",
       "cityCode             0\n",
       "cityPartRange        0\n",
       "numPrevOwners        0\n",
       "made                 0\n",
       "isNewBuilt           0\n",
       "hasStormProtector    0\n",
       "basement             0\n",
       "attic                0\n",
       "garage               0\n",
       "hasStorageRoom       0\n",
       "hasGuestRoom         0\n",
       "price                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47da54cd-d187-49ca-8760-e62a9d477109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>squareMeters</th>\n",
       "      <th>numberOfRooms</th>\n",
       "      <th>hasYard</th>\n",
       "      <th>hasPool</th>\n",
       "      <th>floors</th>\n",
       "      <th>cityCode</th>\n",
       "      <th>cityPartRange</th>\n",
       "      <th>numPrevOwners</th>\n",
       "      <th>made</th>\n",
       "      <th>isNewBuilt</th>\n",
       "      <th>hasStormProtector</th>\n",
       "      <th>basement</th>\n",
       "      <th>attic</th>\n",
       "      <th>garage</th>\n",
       "      <th>hasStorageRoom</th>\n",
       "      <th>hasGuestRoom</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75523</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>9373</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4313</td>\n",
       "      <td>9005</td>\n",
       "      <td>956</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7559081.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80771</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>39381</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3653</td>\n",
       "      <td>2436</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8085989.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55712</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>34457</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2937</td>\n",
       "      <td>8852</td>\n",
       "      <td>135</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5574642.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32316</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>27939</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>659</td>\n",
       "      <td>7141</td>\n",
       "      <td>359</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3232561.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70429</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>38045</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8435</td>\n",
       "      <td>2429</td>\n",
       "      <td>292</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7055052.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>39223</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>39489</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>4552</td>\n",
       "      <td>757</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3926647.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>58682</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>6450</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1995</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5930</td>\n",
       "      <td>9453</td>\n",
       "      <td>848</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5876376.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>86929</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>98155</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6326</td>\n",
       "      <td>4748</td>\n",
       "      <td>654</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>8696869.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>51522</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>9047</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>632</td>\n",
       "      <td>5792</td>\n",
       "      <td>807</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5154055.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39686</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>71019</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5198</td>\n",
       "      <td>5342</td>\n",
       "      <td>591</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3970892.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   squareMeters  numberOfRooms  hasYard  hasPool  floors  cityCode  \\\n",
       "0         75523              3        0        1      63      9373   \n",
       "1         80771             39        1        1      98     39381   \n",
       "2         55712             58        0        1      19     34457   \n",
       "3         32316             47        0        0       6     27939   \n",
       "4         70429             19        1        1      90     38045   \n",
       "5         39223             36        0        1      17     39489   \n",
       "6         58682             10        1        1      99      6450   \n",
       "7         86929            100        1        0      11     98155   \n",
       "8         51522              3        0        0      61      9047   \n",
       "9         39686             42        0        0      15     71019   \n",
       "\n",
       "   cityPartRange  numPrevOwners  made  isNewBuilt  hasStormProtector  \\\n",
       "0              3              8  2005           0                  1   \n",
       "1              8              6  2015           1                  0   \n",
       "2              6              8  2021           0                  0   \n",
       "3             10              4  2012           0                  1   \n",
       "4              3              7  1990           1                  0   \n",
       "5              8              6  2012           0                  1   \n",
       "6             10              9  1995           1                  1   \n",
       "7              3              4  2003           1                  0   \n",
       "8              8              3  2012           1                  1   \n",
       "9              5              8  2021           1                  1   \n",
       "\n",
       "   basement  attic  garage  hasStorageRoom  hasGuestRoom      price  \n",
       "0      4313   9005     956               0             7  7559081.5  \n",
       "1      3653   2436     128               1             2  8085989.5  \n",
       "2      2937   8852     135               1             9  5574642.1  \n",
       "3       659   7141     359               0             3  3232561.2  \n",
       "4      8435   2429     292               1             4  7055052.0  \n",
       "5      2009   4552     757               0             1  3926647.2  \n",
       "6      5930   9453     848               0             5  5876376.5  \n",
       "7      6326   4748     654               0            10  8696869.3  \n",
       "8       632   5792     807               1             5  5154055.2  \n",
       "9      5198   5342     591               1             3  3970892.1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b25873bd-a399-4070-8c77-61a63c2f8150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of       squareMeters  numberOfRooms  hasYard  hasPool  floors  cityCode  \\\n",
       "0            75523              3        0        1      63      9373   \n",
       "1            80771             39        1        1      98     39381   \n",
       "2            55712             58        0        1      19     34457   \n",
       "3            32316             47        0        0       6     27939   \n",
       "4            70429             19        1        1      90     38045   \n",
       "...            ...            ...      ...      ...     ...       ...   \n",
       "9995          1726             89        0        1       5     73133   \n",
       "9996         44403             29        1        1      12     34606   \n",
       "9997         83841              3        0        0      69     80933   \n",
       "9998         59036             70        0        0      96     55856   \n",
       "9999          1440             84        0        0      49     18412   \n",
       "\n",
       "      cityPartRange  numPrevOwners  made  isNewBuilt  hasStormProtector  \\\n",
       "0                 3              8  2005           0                  1   \n",
       "1                 8              6  2015           1                  0   \n",
       "2                 6              8  2021           0                  0   \n",
       "3                10              4  2012           0                  1   \n",
       "4                 3              7  1990           1                  0   \n",
       "...             ...            ...   ...         ...                ...   \n",
       "9995              7              6  2009           0                  1   \n",
       "9996              9              4  1990           0                  1   \n",
       "9997             10             10  2005           1                  1   \n",
       "9998              1              3  2010           0                  1   \n",
       "9999              6             10  1994           1                  0   \n",
       "\n",
       "      basement  attic  garage  hasStorageRoom  hasGuestRoom      price  \n",
       "0         4313   9005     956               0             7  7559081.5  \n",
       "1         3653   2436     128               1             2  8085989.5  \n",
       "2         2937   8852     135               1             9  5574642.1  \n",
       "3          659   7141     359               0             3  3232561.2  \n",
       "4         8435   2429     292               1             4  7055052.0  \n",
       "...        ...    ...     ...             ...           ...        ...  \n",
       "9995      9311   1698     218               0             4   176425.9  \n",
       "9996      9061   1742     230               0             0  4448474.0  \n",
       "9997      8304   7730     345               1             9  8390030.5  \n",
       "9998      2590   6174     339               1             4  5905107.0  \n",
       "9999      8485   2024     278               1             6   146708.4  \n",
       "\n",
       "[10000 rows x 17 columns]>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85dd581a-3582-4b10-a69b-bb6a6485738a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(10313.5)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['price'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbf48337-df4c-4de1-84c5-ab6528d4325d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(10006771.2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['price'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ef92441-00d1-4d6d-8758-6e711c8d4da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns = ['price'])\n",
    "y = df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "40f7edcc-cb5a-40d9-8d6e-4dd2f13c9075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7433cfd4-4235-4233-b5af-2269c9310812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_log = np.log1p(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a13e8e18-2c1f-475f-8a6e-7ba80a5c4acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_scaled, y_log, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "df62d997-6a76-465a-8f17-92d7fbb2d2e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayzek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 52.4491 - mae: 5.1278 - val_loss: 2.6796 - val_mae: 1.2985\n",
      "Epoch 2/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.2226 - mae: 1.1837 - val_loss: 1.8639 - val_mae: 1.0721\n",
      "Epoch 3/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.5699 - mae: 0.9903 - val_loss: 1.4084 - val_mae: 0.9228\n",
      "Epoch 4/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1951 - mae: 0.8620 - val_loss: 1.1583 - val_mae: 0.8310\n",
      "Epoch 5/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9602 - mae: 0.7687 - val_loss: 0.9332 - val_mae: 0.7482\n",
      "Epoch 6/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7931 - mae: 0.6953 - val_loss: 0.7790 - val_mae: 0.6761\n",
      "Epoch 7/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6709 - mae: 0.6347 - val_loss: 0.6682 - val_mae: 0.6186\n",
      "Epoch 8/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5761 - mae: 0.5814 - val_loss: 0.5850 - val_mae: 0.5746\n",
      "Epoch 9/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5014 - mae: 0.5378 - val_loss: 0.5211 - val_mae: 0.5377\n",
      "Epoch 10/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4470 - mae: 0.5021 - val_loss: 0.4655 - val_mae: 0.5105\n",
      "Epoch 11/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4017 - mae: 0.4709 - val_loss: 0.4254 - val_mae: 0.4715\n",
      "Epoch 12/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3669 - mae: 0.4448 - val_loss: 0.3927 - val_mae: 0.4606\n",
      "Epoch 13/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3356 - mae: 0.4201 - val_loss: 0.4059 - val_mae: 0.4482\n",
      "Epoch 14/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3159 - mae: 0.4040 - val_loss: 0.3535 - val_mae: 0.4237\n",
      "Epoch 15/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2998 - mae: 0.3896 - val_loss: 0.3463 - val_mae: 0.4413\n",
      "Epoch 16/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2870 - mae: 0.3790 - val_loss: 0.3446 - val_mae: 0.4036\n",
      "Epoch 17/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2762 - mae: 0.3681 - val_loss: 0.3256 - val_mae: 0.3913\n",
      "Epoch 18/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2738 - mae: 0.3674 - val_loss: 0.3175 - val_mae: 0.3864\n",
      "Epoch 19/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2642 - mae: 0.3584 - val_loss: 0.3406 - val_mae: 0.4038\n",
      "Epoch 20/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2619 - mae: 0.3563 - val_loss: 0.3124 - val_mae: 0.3799\n",
      "Epoch 21/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2615 - mae: 0.3566 - val_loss: 0.2982 - val_mae: 0.3700\n",
      "Epoch 22/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2552 - mae: 0.3504 - val_loss: 0.3107 - val_mae: 0.3778\n",
      "Epoch 23/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2513 - mae: 0.3457 - val_loss: 0.3080 - val_mae: 0.4001\n",
      "Epoch 24/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2505 - mae: 0.3454 - val_loss: 0.2963 - val_mae: 0.3516\n",
      "Epoch 25/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2445 - mae: 0.3389 - val_loss: 0.2954 - val_mae: 0.3575\n",
      "Epoch 26/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2048 - mae: 0.2973 - val_loss: 0.2009 - val_mae: 0.2737\n",
      "Epoch 27/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1409 - mae: 0.2345 - val_loss: 0.1579 - val_mae: 0.2419\n",
      "Epoch 28/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1212 - mae: 0.2214 - val_loss: 0.1409 - val_mae: 0.2222\n",
      "Epoch 29/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1115 - mae: 0.2128 - val_loss: 0.1331 - val_mae: 0.2123\n",
      "Epoch 30/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1058 - mae: 0.2068 - val_loss: 0.1280 - val_mae: 0.2125\n",
      "Epoch 31/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0988 - mae: 0.1987 - val_loss: 0.1189 - val_mae: 0.2213\n",
      "Epoch 32/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0941 - mae: 0.1961 - val_loss: 0.1151 - val_mae: 0.2130\n",
      "Epoch 33/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0885 - mae: 0.1891 - val_loss: 0.1068 - val_mae: 0.1988\n",
      "Epoch 34/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0847 - mae: 0.1849 - val_loss: 0.1083 - val_mae: 0.1994\n",
      "Epoch 35/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0837 - mae: 0.1858 - val_loss: 0.1026 - val_mae: 0.2015\n",
      "Epoch 36/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0827 - mae: 0.1855 - val_loss: 0.0957 - val_mae: 0.1827\n",
      "Epoch 37/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0791 - mae: 0.1805 - val_loss: 0.0974 - val_mae: 0.1880\n",
      "Epoch 38/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0781 - mae: 0.1803 - val_loss: 0.0968 - val_mae: 0.1961\n",
      "Epoch 39/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0753 - mae: 0.1778 - val_loss: 0.0960 - val_mae: 0.1837\n",
      "Epoch 40/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0726 - mae: 0.1731 - val_loss: 0.0898 - val_mae: 0.1766\n",
      "Epoch 41/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0723 - mae: 0.1748 - val_loss: 0.0954 - val_mae: 0.1756\n",
      "Epoch 42/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0709 - mae: 0.1730 - val_loss: 0.0853 - val_mae: 0.1812\n",
      "Epoch 43/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0678 - mae: 0.1684 - val_loss: 0.0929 - val_mae: 0.1780\n",
      "Epoch 44/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0682 - mae: 0.1688 - val_loss: 0.0861 - val_mae: 0.1755\n",
      "Epoch 45/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0653 - mae: 0.1656 - val_loss: 0.0861 - val_mae: 0.1831\n",
      "Epoch 46/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0641 - mae: 0.1629 - val_loss: 0.0831 - val_mae: 0.1756\n",
      "Epoch 47/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0622 - mae: 0.1610 - val_loss: 0.0793 - val_mae: 0.1708\n",
      "Epoch 48/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0606 - mae: 0.1588 - val_loss: 0.0783 - val_mae: 0.1682\n",
      "Epoch 49/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0592 - mae: 0.1570 - val_loss: 0.0739 - val_mae: 0.1527\n",
      "Epoch 50/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0596 - mae: 0.1607 - val_loss: 0.0809 - val_mae: 0.1808\n",
      "Epoch 51/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0570 - mae: 0.1541 - val_loss: 0.0784 - val_mae: 0.1748\n",
      "Epoch 52/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0551 - mae: 0.1511 - val_loss: 0.0787 - val_mae: 0.1714\n",
      "Epoch 53/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0549 - mae: 0.1518 - val_loss: 0.0760 - val_mae: 0.1694\n",
      "Epoch 54/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0515 - mae: 0.1469 - val_loss: 0.0769 - val_mae: 0.1661\n",
      "Epoch 55/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0513 - mae: 0.1463 - val_loss: 0.0775 - val_mae: 0.1771\n",
      "Epoch 56/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0492 - mae: 0.1429 - val_loss: 0.0705 - val_mae: 0.1498\n",
      "Epoch 57/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0462 - mae: 0.1370 - val_loss: 0.0888 - val_mae: 0.1969\n",
      "Epoch 58/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0488 - mae: 0.1452 - val_loss: 0.0726 - val_mae: 0.1524\n",
      "Epoch 59/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0439 - mae: 0.1363 - val_loss: 0.0699 - val_mae: 0.1488\n",
      "Epoch 60/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0428 - mae: 0.1356 - val_loss: 0.0672 - val_mae: 0.1500\n",
      "Epoch 61/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0409 - mae: 0.1324 - val_loss: 0.0672 - val_mae: 0.1587\n",
      "Epoch 62/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0407 - mae: 0.1315 - val_loss: 0.0670 - val_mae: 0.1541\n",
      "Epoch 63/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0387 - mae: 0.1306 - val_loss: 0.0639 - val_mae: 0.1427\n",
      "Epoch 64/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0374 - mae: 0.1252 - val_loss: 0.0572 - val_mae: 0.1329\n",
      "Epoch 65/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0349 - mae: 0.1225 - val_loss: 0.0583 - val_mae: 0.1361\n",
      "Epoch 66/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0346 - mae: 0.1216 - val_loss: 0.0553 - val_mae: 0.1262\n",
      "Epoch 67/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0349 - mae: 0.1228 - val_loss: 0.0573 - val_mae: 0.1373\n",
      "Epoch 68/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0319 - mae: 0.1177 - val_loss: 0.0594 - val_mae: 0.1406\n",
      "Epoch 69/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0310 - mae: 0.1165 - val_loss: 0.0511 - val_mae: 0.1160\n",
      "Epoch 70/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0307 - mae: 0.1164 - val_loss: 0.0617 - val_mae: 0.1522\n",
      "Epoch 71/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0290 - mae: 0.1124 - val_loss: 0.0528 - val_mae: 0.1294\n",
      "Epoch 72/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0278 - mae: 0.1097 - val_loss: 0.0514 - val_mae: 0.1209\n",
      "Epoch 73/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0290 - mae: 0.1153 - val_loss: 0.0539 - val_mae: 0.1317\n",
      "Epoch 74/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0277 - mae: 0.1108 - val_loss: 0.0456 - val_mae: 0.1104\n",
      "Epoch 75/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0257 - mae: 0.1076 - val_loss: 0.0532 - val_mae: 0.1298\n",
      "Epoch 76/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0272 - mae: 0.1123 - val_loss: 0.0460 - val_mae: 0.1163\n",
      "Epoch 77/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0226 - mae: 0.1004 - val_loss: 0.0516 - val_mae: 0.1359\n",
      "Epoch 78/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0231 - mae: 0.1027 - val_loss: 0.0517 - val_mae: 0.1364\n",
      "Epoch 79/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0242 - mae: 0.1064 - val_loss: 0.0415 - val_mae: 0.1125\n",
      "Epoch 80/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0223 - mae: 0.1010 - val_loss: 0.0456 - val_mae: 0.1199\n",
      "Epoch 81/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0209 - mae: 0.0978 - val_loss: 0.0403 - val_mae: 0.1052\n",
      "Epoch 82/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0219 - mae: 0.1011 - val_loss: 0.0465 - val_mae: 0.1199\n",
      "Epoch 83/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0199 - mae: 0.0960 - val_loss: 0.0451 - val_mae: 0.1272\n",
      "Epoch 84/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0220 - mae: 0.1023 - val_loss: 0.0405 - val_mae: 0.0995\n",
      "Epoch 85/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0194 - mae: 0.0939 - val_loss: 0.0434 - val_mae: 0.1217\n",
      "Epoch 86/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0192 - mae: 0.0960 - val_loss: 0.0417 - val_mae: 0.1140\n",
      "Epoch 87/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0173 - mae: 0.0882 - val_loss: 0.0407 - val_mae: 0.1095\n",
      "Epoch 88/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0191 - mae: 0.0967 - val_loss: 0.0412 - val_mae: 0.1130\n",
      "Epoch 89/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0195 - mae: 0.0969 - val_loss: 0.0551 - val_mae: 0.1559\n",
      "Epoch 90/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0176 - mae: 0.0911 - val_loss: 0.0354 - val_mae: 0.1016\n",
      "Epoch 91/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0166 - mae: 0.0873 - val_loss: 0.0426 - val_mae: 0.1147\n",
      "Epoch 92/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0173 - mae: 0.0905 - val_loss: 0.0364 - val_mae: 0.0910\n",
      "Epoch 93/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0167 - mae: 0.0908 - val_loss: 0.0445 - val_mae: 0.1149\n",
      "Epoch 94/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0166 - mae: 0.0881 - val_loss: 0.0386 - val_mae: 0.1034\n",
      "Epoch 95/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0149 - mae: 0.0855 - val_loss: 0.0348 - val_mae: 0.0986\n",
      "Epoch 96/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0159 - mae: 0.0879 - val_loss: 0.0348 - val_mae: 0.0975\n",
      "Epoch 97/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0153 - mae: 0.0855 - val_loss: 0.0367 - val_mae: 0.1065\n",
      "Epoch 98/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0142 - mae: 0.0824 - val_loss: 0.0356 - val_mae: 0.1073\n",
      "Epoch 99/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0153 - mae: 0.0867 - val_loss: 0.0356 - val_mae: 0.0887\n",
      "Epoch 100/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0132 - mae: 0.0791 - val_loss: 0.0440 - val_mae: 0.1367\n",
      "Epoch 101/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0156 - mae: 0.0867 - val_loss: 0.0355 - val_mae: 0.0879\n",
      "Epoch 102/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0134 - mae: 0.0813 - val_loss: 0.0400 - val_mae: 0.1106\n",
      "Epoch 103/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0152 - mae: 0.0868 - val_loss: 0.0357 - val_mae: 0.0933\n",
      "Epoch 104/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0143 - mae: 0.0839 - val_loss: 0.0393 - val_mae: 0.1058\n",
      "Epoch 105/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0134 - mae: 0.0811 - val_loss: 0.0347 - val_mae: 0.0935\n",
      "Epoch 106/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0126 - mae: 0.0780 - val_loss: 0.0325 - val_mae: 0.0977\n",
      "Epoch 107/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0136 - mae: 0.0828 - val_loss: 0.0399 - val_mae: 0.1215\n",
      "Epoch 108/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0127 - mae: 0.0791 - val_loss: 0.0341 - val_mae: 0.0977\n",
      "Epoch 109/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0130 - mae: 0.0813 - val_loss: 0.0418 - val_mae: 0.1115\n",
      "Epoch 110/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0124 - mae: 0.0791 - val_loss: 0.0291 - val_mae: 0.0887\n",
      "Epoch 111/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0135 - mae: 0.0833 - val_loss: 0.0311 - val_mae: 0.0859\n",
      "Epoch 112/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0109 - mae: 0.0742 - val_loss: 0.0284 - val_mae: 0.0900\n",
      "Epoch 113/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0139 - mae: 0.0830 - val_loss: 0.0302 - val_mae: 0.0844\n",
      "Epoch 114/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0133 - mae: 0.0809 - val_loss: 0.0381 - val_mae: 0.1063\n",
      "Epoch 115/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0129 - mae: 0.0798 - val_loss: 0.0321 - val_mae: 0.0985\n",
      "Epoch 116/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0110 - mae: 0.0742 - val_loss: 0.0284 - val_mae: 0.0919\n",
      "Epoch 117/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0114 - mae: 0.0755 - val_loss: 0.0338 - val_mae: 0.0918\n",
      "Epoch 118/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0130 - mae: 0.0807 - val_loss: 0.0278 - val_mae: 0.0820\n",
      "Epoch 119/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0127 - mae: 0.0800 - val_loss: 0.0311 - val_mae: 0.0880\n",
      "Epoch 120/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0105 - mae: 0.0737 - val_loss: 0.0303 - val_mae: 0.0796\n",
      "Epoch 121/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0114 - mae: 0.0756 - val_loss: 0.0298 - val_mae: 0.0855\n",
      "Epoch 122/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0107 - mae: 0.0732 - val_loss: 0.0298 - val_mae: 0.0847\n",
      "Epoch 123/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0118 - mae: 0.0773 - val_loss: 0.0350 - val_mae: 0.1074\n",
      "Epoch 124/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0116 - mae: 0.0772 - val_loss: 0.0305 - val_mae: 0.0830\n",
      "Epoch 125/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0091 - mae: 0.0665 - val_loss: 0.0351 - val_mae: 0.0913\n",
      "Epoch 126/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0102 - mae: 0.0713 - val_loss: 0.0289 - val_mae: 0.0872\n",
      "Epoch 127/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0114 - mae: 0.0758 - val_loss: 0.0283 - val_mae: 0.0913\n",
      "Epoch 128/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0103 - mae: 0.0726 - val_loss: 0.0289 - val_mae: 0.0853\n",
      "Epoch 129/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0120 - mae: 0.0772 - val_loss: 0.0346 - val_mae: 0.1152\n",
      "Epoch 130/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0096 - mae: 0.0702 - val_loss: 0.0274 - val_mae: 0.0789\n",
      "Epoch 131/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0106 - mae: 0.0727 - val_loss: 0.0281 - val_mae: 0.0947\n",
      "Epoch 132/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0106 - mae: 0.0727 - val_loss: 0.0322 - val_mae: 0.0893\n",
      "Epoch 133/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0101 - mae: 0.0711 - val_loss: 0.0327 - val_mae: 0.0932\n",
      "Epoch 134/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0098 - mae: 0.0700 - val_loss: 0.0257 - val_mae: 0.0805\n",
      "Epoch 135/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0097 - mae: 0.0693 - val_loss: 0.0341 - val_mae: 0.1001\n",
      "Epoch 136/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0126 - mae: 0.0773 - val_loss: 0.0278 - val_mae: 0.0823\n",
      "Epoch 137/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0096 - mae: 0.0699 - val_loss: 0.0315 - val_mae: 0.0911\n",
      "Epoch 138/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0095 - mae: 0.0693 - val_loss: 0.0252 - val_mae: 0.0708\n",
      "Epoch 139/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0105 - mae: 0.0711 - val_loss: 0.0247 - val_mae: 0.0699\n",
      "Epoch 140/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0105 - mae: 0.0711 - val_loss: 0.0276 - val_mae: 0.0914\n",
      "Epoch 141/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0110 - mae: 0.0732 - val_loss: 0.0257 - val_mae: 0.0778\n",
      "Epoch 142/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0087 - mae: 0.0674 - val_loss: 0.0329 - val_mae: 0.1079\n",
      "Epoch 143/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0108 - mae: 0.0749 - val_loss: 0.0271 - val_mae: 0.0811\n",
      "Epoch 144/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0091 - mae: 0.0680 - val_loss: 0.0321 - val_mae: 0.1078\n",
      "Epoch 145/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0091 - mae: 0.0667 - val_loss: 0.0290 - val_mae: 0.0929\n",
      "Epoch 146/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0101 - mae: 0.0708 - val_loss: 0.0300 - val_mae: 0.0969\n",
      "Epoch 147/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0109 - mae: 0.0741 - val_loss: 0.0311 - val_mae: 0.0800\n",
      "Epoch 148/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0096 - mae: 0.0706 - val_loss: 0.0274 - val_mae: 0.0841\n",
      "Epoch 149/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0085 - mae: 0.0656 - val_loss: 0.0255 - val_mae: 0.0794\n",
      "Epoch 150/150\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0086 - mae: 0.0664 - val_loss: 0.0254 - val_mae: 0.0949\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "nn = neural(layers=[64, 32, 1], activations=[\"relu\", \"relu\", \"linear\"], optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "nn.build_model(input_dim)\n",
    "nn.compile()\n",
    "history = nn.fit(X_train, y_train, batch_size=32, epochs=150, val_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
